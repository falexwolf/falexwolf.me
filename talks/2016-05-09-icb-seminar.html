<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">

  <title>Convolutional Neural Networks</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  
  <link rel="stylesheet" href="reveal.js-3.3.0/css/reveal.css">
  <link rel="stylesheet" href="2016-05-09-icb-seminar/mywhite.css" id="theme"> 
  
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    <!-- if "print-pdf" is in the querry, use the pdf.css style sheet (in the regexp: gi=global,case-insensitive) -->
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js-3.3.0/css/print/pdf.css' : 'reveal.js-3.3.0/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  
</head>

<style type="text/css">
/* bug fix : prevent firefox from changing background to black in full screen */
html { background-color: white !important; }
</style>

<body>  
<div class="reveal">
  <div class="slides">

    <section>
      <div class="slidebds"></div>
      <h3>Convolutional Neural Networks</h3>
      <div style="position:absolute; top:60px">
        <p style="font-size:70%"> pedagogic talk based on
          <cit style="font-size:80%">Goodfellow, Bengio & Courville, <i>Deep Learning</i> (2016, Ch. 9)</cit>
          <br>
          and <cit style="font-size:80%">deeplearning.net/tutorial/lenet.html</cit>
        </p>
      </div>
      
      <!-- there is some margin at the bottom on purpose: otherwise in pdf export,
           the icons appear on the next slide -->
      <div style="position:absolute; top:446px; width:1100px">
        <p style="text-align:left">Deep Learning Seminar · ICB · Helmholtz Munich · 9 May 2016</p>
        <br>
        <p>F. Alexander Wolf | <a href="http://falexwolf.de"><cit>falexwolf.de</cit></a>
        <p>Institute of Computational Biology</p>
        <p>Helmholtz Munich</p>
      </div>
      <div style="position:absolute; right:355px; top:608px; height:60px">
        <img data-src="logos/helmholtz.png">
      </div>
      <div style="position:absolute; right:0px; top:619.5px; height:100px">
    	<img data-src="logos/helmholtzmunich.svg">
      </div>

      <!-- Notes on usage -->
      <div style="position:absolute; top:720px; right:0px; font-size:50%; color:grey">
      fullscreen: 'f' / navigation: arrow keys / black screen: 'b' / overview: 'o'
      </div>
        
    </section>

    <!-- <section> -->
    <!--   <div id="slidebds"></div> -->
      
    <!--   <h3>Convolutional Neural Networks</h3> -->
    <!--   <div style="position:absolute; top:60px"> -->
    <!--     <p style="font-size:70%"> pedagogic talk based on -->
    <!--       <cit style="font-size:80%">Goodfellow, Bengio & Courville, <i>Deep Learning</i> (2016, Ch. 9)</cit> -->
    <!--       <br> -->
    <!--       and <cit style="font-size:80%">deeplearning.net/tutorial/lenet.html</cit> -->
    <!--       <cit style="font-size:80%"></cit> -->
    <!--     </p> -->
    <!--   </div> -->
      
    <!--   <div style="position:absolute; top:515px"> -->
    <!--     <p>F. Alex Wolf</p> -->
    <!--     <p>Deep Learning Seminar | May 9th 2016</p> -->
    <!--     <p>Institute of Computational Biology</p> -->
    <!--     <p>Helmholtz Zentrum München</p> -->
    <!--   </div> -->
    <!--   <div style="position:absolute; right:355px; top:638px; height:60px"> -->
    <!--     <img data-src="2016-05-09-icb-seminar/../logos/helmholtz.jpg"> -->
    <!--   </div> -->
    <!--   <div style="position:absolute; right:0px; top:648px; height:100px"> -->
    <!-- 	<img data-src="2016-05-09-icb-seminar/../logos/helmholtzmunich.svg"> -->
    <!--   </div> -->
    <!-- </section> -->

    <section>
      <div id="slidebds"></div>
      <h3>Motivation</h3>
      <div style="position: absolute; left: 0px; top: 80px">
        <p>
          <cit style="font-size:100%"> Hubel and Wiesel (1959, 1962, 1968)</cit>:
          Nobel prize 1981 for work on mammalian vision system.
          <span class="fragment">
            &nbsp; &#9655; Results on primary visual cortex (V1).</span>
        </p>
      </div>

      <div style="position: absolute; left: 0px; top: 180px">
        <ul style="line-height:1.5">
          <li class="fragment">
            V1 is arranged in a spatial map mirroring the structure of the image
            in the retina.
          </li>
          <li class="fragment">
            V1 has <i>simple cells</i> whose activity is a linear function of
            the image in a small localized <i>receptive field</i>.
          </li>
          <li class="fragment">
            V1 has <i>complex cells</i> whose activity is invariant to small
            spatial translations.
          </li>
          <li class="fragment" style="color:#AA0000">
            Neurons in V1 respond most strongly to very
            specific, simple patterns of light, such as oriented bars, but
            respond hardly to any other patterns.
          </li>
        </ul>
      </div>

      <!-- Mention 2014 Nobel prize for O'Keefe and Mosers for their work on
           positioning system.  Mention that there are many differences between
           a CNN and the human vision system!! 
      -->
      
    </section>

    
    <section>
      <div id="slidebds"></div>
      
      <h3>Recap</h3>

      <div style="position: absolute; left: 0px; top: 80px">
        <ul style="line-height:1.5">
          <li class="fragment"> A <i> Multilayer perceptron </i> = <i> Feedforward Neural
              Network </i> is a probabilisitic model: <i>layered</i> matrix-multiplications stacked
              with non-linear activation functions. </li>
          <li class="fragment"> Optimize log likelihood (classification or
            regression error) by stochastic gradient descent.
            Full gradient by <i>backpropagating</i> layer gradients. </li>
        </ul>
      </div>

      <div class="fragment" style="position: absolute; left: 0px; top: 470px">
        <h4>So, what is a <i>Convolutional Neural Network</i>?</h4>
        <ul style="line-height:1.5">
          <li class="fragment"> It's simply a neural network that
            uses <i>convolution</i> 
            in place of a "general matrix multiplication" in
            at least one of its layers.
            <br>
            <cit style="font-size:70%">
              LeCun, Bottou, Bengio & Haffner, Proc. IEEE <b>86</b> 2278 (1998) </cit>
          </li>
        </ul>
      </div>
      
    </section>
    
    <section>
      <div id="slidebds"></div>
      
      <h3>What is a <i>convolution</i>?</h3>

      <div style="position:absolute; top:80px">
        <ul style="line-height:1.8">
          <li class="fragment"> convolution of functions $f(t)$ and $w(t)$
            $$ (f * w)(t) = \int_{-\infty}^\infty d\tau\, w(t-\tau)\, f(\tau) $$
          </li>
          <li class="fragment"> similar to cross correlation of $f(t)$ and $w(t)$
            $$ (f \star w)(t) = \int_{-\infty}^\infty d\tau\, w(t+\tau)\, f(\tau)$$
          </li>
        </ul>
      </div>
      
      <div class="fragment" style="width:1100px; line-height:1.5">
        <p style="margin-left:47px; position:absolute; top:450px">
          &#9655;&nbsp;
          </p><p style="margin-left:80px; position:absolute; top:450px">
          In deep learning, both of these operations are used in its discrete form
          and referred to as "convolution".
        </p>
        <div style="margin-left:80px; position:absolute; top:570px">
          $$ \quad\,\, (f * w)_t = \sum_\tau  w_{t-\tau}\, f_\tau$$
        </div>  
      </div>
        
    </section>

    
    <section>
      <div id="slidebds"></div>
      
      <h3>Convolution as constraint matrix multiplication </h3>

      <div style="position:absolute; top:70px">
        <ul style="line-height:1.5">
          <li class="fragment"> <div style="margin-bottom:30px">
            discrete convolution of functions $f_t$ and
            $w_t$, $t\in\{1,2,...,D\}$,</div>
            $$ \mathbf{\tilde f} = \sum_\tau w_{t-\tau}\, f_\tau
               = \mathbf{W} \mathbf{f}, \quad \mathbf{\tilde f},\mathbf{f} \in \mathbb{R}^D $$
            where $W_{t\tau} = w_{t-\tau}$, $\mathbf{W} \in
            \mathbb{R}^{D\times D}$.
          </li>
        </ul>
      </div>
        
      <div class="fragment" style="width:1100px; line-height:1.5">
        <p style="margin-left:80px; position:absolute; top:320px">
          &#9655;&nbsp;
          Instead of $D^2$, only $D$ <i>independent</i> components.
        </p>
      </div>
        
      <div class="fragment" style="position:absolute; top:400px">
        <h4>Natural extension: sparsity</h4>
        <ul style="line-height:1.5">        
        <li class="fragment">
          demand: $w_{t-\tau} \stackrel{!}{=} 0$ for $|t-\tau| > d$
          <span class="fragment"><br>
          [usual property of kernels: e.g. Gaussian $ W_{t\tau} =
          e^{-\frac{(t-\tau)^2}{2d^2}}$] </span>
        </li>
        </ul>
      </div>        
      <div class="fragment" style="width:1100px; line-height:1.5">
        <p style="margin-left:80px; position:absolute; top:620px">
          &#9655;&nbsp;
          Instead of $D^2$, only $2d$ nonzero components.
          <span class="fragment"> &nbsp;&#9655;&nbsp; Statistics &#9786;!
            </span>
        </p>
      </div>
      
    </section>

    <section>
      <div id="slidebds"></div>
      
      <h3>Convolution as sparsity constraint: graphical </h3>

      <div style="position:absolute; top:150px; left:60px; line-height:1.5">
        general weight matrix $\mathbf{W}$
        <br> <span style="font-size:70%"> (arrows represent arbitrary values) </span>
      </div>
      
      <div style="position:absolute; top:80px; left:495px; height:300px">
         <img data-src="2016-05-09-icb-seminar/figs/sparse2.png">
      </div>

      <div style="position:absolute; top:100px; left:950px; line-height:1.5">
        $\mathbf{\tilde f}$ </div>
      <div style="position:absolute; top:235px; left:950px; line-height:1.5">
        $\mathbf{f}$ </div>

      <div style="position:absolute; top:300px; left:550px; font-size:70%">
        <i>receptive field</i> of $\tilde f_t$: full range $D$</div>

      <div style="position:absolute; top:460px; left:5px; line-height:1.5">
        convolution type $\mathbf{W}$
        <br> <span style="font-size:70%;line-height:0.6"> (arrows: same values across <i>receptive fields</i> ) </span>
      </div>
      
      <div style="position:absolute; top:380px; left:495px; height:300px">
         <img data-src="2016-05-09-icb-seminar/figs/sparse1.png">
      </div>

      <div style="position:absolute; top:410px; left:950px; line-height:1.5">
        $\mathbf{\tilde f}$ </div>
      <div style="position:absolute; top:545px; left:950px; line-height:1.5">
        $\mathbf{f}$ </div>

      <div style="position:absolute; top:620px; left:550px; font-size:70%">
        <i>receptive field</i> of $\tilde f_t$: local range $2d$</div> 
      
    </section>

    
    <section>
      <div id="slidebds"></div>
      
      <h3>When is convolution a useful sparsity constraint?</h3>

      <div class="fragment" style="position:absolute; top:80px">
      <p style="margin-bottom:30px">Consider an example ($d=1$)</p>
      <p style="margin-left:60px"> $ \mathbf{W} = \left(\begin{array}{ccccc}
            \ddots & -1 & 1 & 0 & \ddots\\
            \ddots & 0 & -1 & 1 & \ddots \end{array} \right)$
          $\,\Leftrightarrow\,$ $\tilde f_t = f_t - f_{t-1}$,
      </p>
      </div>
      
      <p style="position:absolute; top:400px; left:60px" class="fragment">
        that is, $\,\,\mathbf{f}$ = </p>
      <div style="position:absolute; top:320px; left:240px; height:200px" class="fragment">
        <img data-src="2016-05-09-icb-seminar/figs/dog1.png" > </div>
      
      <p style="position:absolute; top:400px; left:480px" class="fragment">
          $\mapsto\,\, \mathbf{\tilde f}$ = </p>
      <div style="position:absolute; top:320px; left:610px; height:200px" class="fragment">
         <img data-src="2016-05-09-icb-seminar/figs/dog2.png" > </div>

      <div class="fragment" style="width:1100px; line-height:1.5">
        <p style="margin-left:80px; position:absolute; top:580px">
          &#9655;&nbsp;
            Simple edge structures are revealed!
        </p>
      </div>

      
    </section>

    <section>
      <div id="slidebds"></div>
      
      <h3>When is convolution a useful sparsity constraint?</h3>

      <div style="width:1100px; line-height:1.5; position:absolute; top:90px">
        <ul>
         <li>
          To obtain $\mathbf{\tilde f}$, the <i>same local linear</i> operation
          is applied to every $t$ and its $d$ neighbors. Here, it's multiplying
          with $\mathbf{w} = (-1, 1)$.
         </li>
         <li class="fragment">
          This is meaningful if data has <i>dependencies</i>
          between <i>degrees of freedom $f_t$</i> that
          appear
          <span style="color:#aa0000;">independent of the index $t$</span>
          and are <span style="color:#aa0000;">constraint locally to distance $d$</span>: 
          data features <i>local patterns</i>.
        </li>
        </ul>
      </div>
      
      <div class="fragment" style="width:1100px; line-height:1.5; position:absolute; top:420px">
        <h4>Examples for such <i>local patterns</i></h4>
        <ul>
         <li> Images: edges
         </li>
         <li> Audio: frequency patterns
         </li>
         <li> Language: grammar structures
         </li>
        </ul>
      </div>

      <p class="fragment" style="width:400px; line-height:1.5; position:absolute; top:450px; left:700px">
        <b>Test:</b> information-content <i>not</i> invariant under permutation?
      </p>
        
    </section>

    <section>
      <div id="slidebds"></div>
      
      <h3>Learn a kernel</h3>
      <div class="fragment" style="position:absolute; top:80px">
      <p style="margin-bottom:30px">
         Let us initialize a kernel ($d=1$) with random values $w_i$</p>
      <p style="margin-left:60px"> $ \mathbf{W} = \left(\begin{array}{ccccc}
            \ddots & w_1 & w_2 & 0 & \ddots\\
            \ddots & 0 & w_1 & w_2 & \ddots \end{array} \right)$
            $\,\Leftrightarrow\,$ $\tilde f_t = w_1 f_t + w_2 f_{t-1}$,
      </p>
      </div>
      <p style="position:absolute; top:400px; left:60px" class="fragment">
        again, $\,\,\mathbf{f}$ = </p>
      <div style="position:absolute; top:320px; left:240px; height:200px" class="fragment">
        <img data-src="2016-05-09-icb-seminar/figs/wolf1.png" > </div>
      
      <p style="position:absolute; top:400px; left:480px" class="fragment">
          $\mapsto\,\, \mathbf{\tilde f}$ = </p>
      <div style="position:absolute; top:320px; left:610px; height:200px" class="fragment">
         <img data-src="2016-05-09-icb-seminar/figs/wolf2.png" > </div>

      <div class="fragment" style="width:1100px; line-height:1.5; position:absolute; top:520px">
        <ul><li>
          Also the random kernel seems to detect edges very well! 
          Most work is already done! &#9655;&nbsp
          It seems not too hard to learn meaningful kernels! 
        </li></ul>
      </div>
      
    </section>

    <section>
      <div id="slidebds"></div>
      
      <h3>Learn several kernels per layer</h3>

      <div style="width:1100px; line-height:1.5; position:absolute; top:80px">
        <ul style="width:550px">
          <li class="fragment"> Several kernels should learn different
            <i>local patterns</i>:
            e.g. edges oriented in different directions.
            <div style="position:absolute; top:0px; left:650px">
              <img data-src="2016-05-09-icb-seminar/figs/cnn_sketch_layer.png"
                   width="400px">
            </div>
          </li>
          <li class="fragment">
            So: evidently it's meaningful to use the same kernel for each
            location $t$ in the input, because patterns appear <i>in the same
            way</i> across locations $t$. 
          </li>
        </ul>
      </div>

      <p style="position:absolute; top:530px; left:630px"
         class="fragment"> 
           <b>But</b>: What about the relevance of
           <i>where patterns appear</i>? </p>
      
    </section>


    <section>
      <div id="slidebds"></div>
      
      <h3>Pooling layers and local translational invariance</h3>
      
      <div class="fragment" style="width:1100px; line-height:1.5; position:absolute; top:80px">
        <h4>Assumption</h4>
        <ul>
          <li>
            In most cases, classification information does not depend strongly 
            on the location (index $t$) of a pattern. 
            That is, the <i>presence of a pattern</i> is more
            important than its <i>location</i>.
          </li>  
          <li class="fragment"> In many cases, our <i>only</i>
            interest is the presence or absence of a pattern.
          </li>
        </ul>
      </div>

      <div class="fragment" style="width:1100px; line-height:1.5; position:absolute; top:450px">
        <h4>Example: shifting the input</h4>
        <ul style="width:550px">
          <li>Pooling layer = implement local translational invariance 
          </li>
          <li class="fragment">here: max pooling layer
            <div style="position:absolute; top:-100px; left:630px">
              <img data-src="2016-05-09-icb-seminar/figs/trans_invariance1.png"
                   width="450px">
            </div>
            <div style="position:absolute; top:60px; left:630px">
              <img data-src="2016-05-09-icb-seminar/figs/trans_invariance2.png"
                   width="450px">
             </div>

          </li>
        </ul>
      </div>

    </section>  
      
    <section>
      <div id="slidebds"></div>
      
      <h3>Assemble everything</h3>

      <div style="position:absolute; top:30px; left:650px">
        <img data-src="2016-05-09-icb-seminar/figs/scheme.png"
             width="400px">
      </div>
      
      <div style="width:1100px; line-height:1.5; position:absolute; top:80px">
        <ol style="width:550px">
          <li class="fragment">
            Read input $\mathbf{f}$. </li>
          <li class="fragment">
            <i>Convolution stage</i> <br>
            $\,\mathbf{\tilde f}^{(k)} := \mathbf{W}^{(k)} \mathbf{f},$
            <br> where $\mathbf{W}^{(k)}$ is one of $K$ <i>convolution kernels</i>,
                 $k=1,...,K$.
          </li>
          <li class="fragment">
            <i>Detector stage</i>
            $\, \tilde f_t^{(k)} := \phi(\tilde f_t^{(k)} + b)$
            where $\phi$ is an activation function, $b$ a bias.
          </li>
          <li class="fragment">
            <i>Pooling stage</i>
            $\, \tilde f_t^{(k)} := \max_{\tau \in [t-d,t+d]} \tilde f_t^{(k)}$
          </li>
        </ol>
      </div>
      
    </section>

    <section>
      <div id="slidebds"></div>
      
      <h3>Some comments</h3>
      
      <div style="width:1100px; line-height:1.5; position:absolute; top:80px">
        <ul style="width:550px">
          <li class="fragment">
            Receptive field can grow layer wise.
            <div style="position:absolute; top:-80px; left:650px">
              <img data-src="2016-05-09-icb-seminar/figs/receptive_field.png"
                   width="400px">
            </div>      
          </li>
          <br>
          <br>
          <li class="fragment">
            Downsampling after pooling layer accounts for reduced information.
            <div style="position:absolute; top:200px; left:650px">
              <img data-src="2016-05-09-icb-seminar/figs/downsampling.png"
                   width="400px">
            </div>      
          </li>
          <br>
          <li class="fragment" style="width:1000px">
            From a Bayesian view, convolutional networks encode our believes
            about the structure of certain data
            - as argued up to here -  
            using an infinitely strong prior.
          </li>
        </ul>
      </div>      
    </section>


        <section>
      <div id="slidebds"></div>
      
      <h3>More comments/questions</h3>
      
      <div style="width:1000px; line-height:1.5; position:absolute; top:80px">
        <ul style="width:1000px">
          <li class="fragment">
            Why do we put <i>fully connected layers</i> on top of the
            convolutional layers? For example, if our classification label
            is translation-invariant, there should be a smarter way?
          </li>
          <li class="fragment">
            Traditionally, CNNs have been used for whole-image
            classification. Recent work deals with their application to
            pixelwise classification (object detection, segmentation, tracking),
            and aims at going beyond and <i>independent</i> treatment of patches.
          </li>
          
        </ul>
      </div>

      <p style="font-size:150%;position:absolute;top:500px;left:470px"
         class="fragment"><b>Thank you!</b></p>
      
    </section>

    
    
  </div>    
</div>

<script src="reveal.js-3.3.0/lib/js/head.min.js"></script>
<script src="reveal.js-3.3.0/js/reveal.js"></script>
<script>
  Reveal.initialize({
    width: 1100,
    height: 700,
    controls: false,
    slideNumber: 'c/t',
    progress: false,
    keyboard: true,
    center: false,
    // set history = true for developing, 
    // consider setting false when finished because this pushes 
    // the hash to the browser history (which is not a problem,
    // but can be a matter of taste)
    history: true,
    fragements: true,
    transition: 'none',
    margin: window.location.search.match( /print-pdf/gi ) ? 0.05 : 0.15,
    viewDistance: 3,

    math: {
      // mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
      config: 'TeX-AMS_HTML-full'
    },

    dependencies: [
      { src: 'reveal.js-3.3.0/lib/js/classList.js' },
      { src: 'reveal.js-3.3.0/plugin/math/math.js', async: true }
    ]
  });
</script>
</body>
</html>
